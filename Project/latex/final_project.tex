% use paper, or submit
% use 11 pt (preferred), 12 pt, or 10 pt only

\documentclass[letterpaper, preprint, paper,11pt]{AAS}	% for preprint proceedings
%\documentclass[letterpaper, paper,11pt]{AAS}		% for final proceedings (20-page limit)
%\documentclass[letterpaper, paper,12pt]{AAS}		% for final proceedings (20-page limit)
%\documentclass[letterpaper, paper,10pt]{AAS}		% for final proceedings (20-page limit)
%\documentclass[letterpaper, submit]{AAS}			% to submit to JAS

\usepackage{bm}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{algpseudocode}
%\usepackage[notref,notcite]{showkeys}  % use this to temporarily show labels
\usepackage[colorlinks=true, pdfstartview=FitV, linkcolor=black, citecolor= black, urlcolor= black]{hyperref}
\usepackage{overcite}
\usepackage{footnpag}			      	% make footnote symbols restart on each page
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{slashbox}
\usepackage{multirow}


\newcommand\scalemath[2]{\scalebox{#1}{\mbox{\ensuremath{\displaystyle #2}}}}
\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}

\PaperNumber{XX-XXX}



\begin{document}

\title{DRONE STATE ESTIMATION USING MULTIPLE INFRARED CAMERAS}

\author{Kadhir Umasankar\thanks{Graduate Student, Daniel Guggenheim School of Aerospace Engineering, kadhir.umasankar@gatech.edu}}


\maketitle{} 		


\begin{abstract}
% TODO: come back at the end
A motion capture system is generally used to estimate drone states in laboratory environments. For this setup, some reflective markers are attached to the drone, and multiple infrared cameras are positioned around the experiment area. The reflective markers' positions in space, along with their relative positions with respect to each other, are captured by the cameras, processed by software on the command center computer and are used to return the 3D position, velocities, and roll, pitch, and yaw of the drone. In this study, the logic used by the software on the command center computer will be replicated. A system that uses multiple cameras (all with their own biases and noise) will be simulated to estimate the state of the drone.
\end{abstract}


\section{Introduction}
Drones generally use GPS measurements in conjunction with measurements from an IMU to estimate their position, velocity, and their roll, pitch, and yaw. In laboratory environments, the drones cannot use GPS data to complement other sensory inputs, so a motion capture system is generally used\cite{vicon_2022}. In such a system, a local origin is set, and the drone uses that as the origin of its inertial frame of reference.

In optical-passive motion capture, retro-reflective markers are placed at various places on the drone. Multiple infrared cameras are positioned around the experiment area, and they capture the infrared light reflected back from the retroreflective markers at rates of around 120 Hz. Such high input frequency is important when testing out controllers, as a high number of samples would allow the controller to control better against sudden changes in the state of the system. Figure \ref{fig:MotionCaptureExample} shows examples of a motion capture space and a drone with retro-reflective markers attached.


\begin{figure}[htb]
\centering
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/ViconMotionCaptureSpace}
	\caption{Example motion capture space setup\cite{ViconMotionCaptureSpace}}
	\label{fig:ViconMotionCaptureSpace}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{Figures/MotionCaptureDrone}
	\caption{Example drone with retro-reflective markers\cite{MotionCaptureDrone}}
	\label{fig:MotionCaptureDrone}
\end{subfigure}
\caption{Example setup that could be used if this study were to be repeated on real-life data}
\label{fig:MotionCaptureExample}
\end{figure}

Most previous studies in drone state estimation have assumed constant inputs. In this study, a linear-quadratic regulator (LQR) controller will be used to find the inputs to the system's dynamics at every timestep, thereby allowing more types of flight paths to be tracked. This study will use simulated motion capture system data to estimate the states of a drone in various flight patterns. The simulation will take place in a Gazebo 3D simulation environment\cite{gazebo}. A ROS (Robot Operating System)\cite{ros} package will be used to perform control and trajectory planning for the drone, and logs of the flight data will be will be run through an Extended Kalman filter (EKF) and an Unscented Kalman filter (UKF) in MATLAB to obtain an accurate estimate of the position of the drone. The performance of the filters on flight paths of varying levels of difficulty will be analyzed.

% TODO: The remainder of the paper is organized as follows.

% TODO: come up with better names later
% TODO: periods at the end of captions?
\section{Simulation Setup}

Data for this study was obtained through simulation. The drone chosen for this experiment was the 3DR Iris\cite{iris}, a model of which is available for use in Gazebo\cite{gazebo}. The Spatial Data File (SDF) of the Iris' model was edited to include four reflective markers. The positions of these markers can be seen in Table \ref{tab:DroneModel} and Figure \ref{fig:DroneModel}. This drone also contained an IMU, and the SDF of the Iris' model showed that it was located at the centroid of the drone.

\begin{table}[htbp]
	\fontsize{10}{10}\selectfont
    \caption{Positions of the reflective markers (in cm) with respect to the centroid of the drone}
   \label{tab:DroneModel}
        \centering 
   \begin{tabular}{c | c | c | c} % Column formatting, 
      \hline 
      Marker Number    & x & y & z \\
      \hline 
      1      & 10.5 & 0 & 0 \\
      2      & 0 & 6.0 & 0 \\
      3      & -11.5 & 0 & 0 \\
      4      & 0 & -6.0 & 0 \\
      \hline
   \end{tabular}
\end{table}

\begin{figure}[htb]
	\centering\includegraphics[width=5in]{Figures/DroneModel}
	\caption{Right and left views of the drone that will be used for simulation}
	\label{fig:DroneModel}
\end{figure}

A world with four Vicon Vantage V16 infrared cameras\cite{vicon_2022} was then created in Gazebo, and their coordinates can be seen in Table \ref{tab:cameras}. Figure \ref{fig:GazeboWorld} shows the setup of the environment. The distance from these cameras to reflective markers on the drone was measured at a rate of approximately 120 Hz, which is consistent with the spec sheet of the Vicon Vantage V16\cite{vicon_2022}.

\begin{table}[htbp]
	\fontsize{10}{10}\selectfont
    \caption{Coordinates of the cameras in m}
   \label{tab:cameras}
        \centering 
   \begin{tabular}{c | c | c | c} % Column formatting, 
      \hline 
      Camera ID    & x & y & z \\
      \hline 
      1      & 10 & 10 & 10 \\
      2      & -10 & 10 & 10 \\
      3      & -10 & -10 & 10 \\
      4      & 10 & -10 & 10 \\
      \hline
   \end{tabular}
\end{table}

\begin{figure}[htb]
	\centering\includegraphics[width=3.5in]{Figures/GazeboWorld}
	\caption{The Gazebo world that the simulation will take place in. Each square has a side length of 1m. Thus, the entire world is 10m $\times$ 10m $\times$ 10m}
	\label{fig:GazeboWorld}
\end{figure}

\section{Dynamics Modeling}

Eq. \eqref{eq:X} shows the states that were to be observed for the filter, where $x$, $y$, and $z$ are the $x$, $y$, and $z$ positions of the drone, $v_x$, $v_y$, and $v_z$ are the $x$, $y$, and $z$ velocities of the drone, $\phi$, $\theta$, and $\psi$ are the roll, pitch, yaw of the drone, and $p$, $q$, and $r$ are the body angular velocities in terms of Euler angles and Euler rates. 
\setcounter{MaxMatrixCols}{12}
\begin{equation}
	\label{eq:X}
	X = \begin{bmatrix}
		x & y & z & v_x & v_y & v_z & \phi & \theta & \psi & p & q & r
\end{bmatrix}^T
\end{equation}

Taking the time derivative of $X_1$ through $X_3$ from Eq. \eqref{eq:X} gives Eq. \eqref{eq:X1dot}.

\begin{equation}
\begin{bmatrix}
\dot{x}\\
\dot{y}\\
\dot{z}
\end{bmatrix} =
\begin{bmatrix}
v_x \\
v_y\\
v_z
\end{bmatrix}
\label{eq:X1dot}
\end{equation}

The velocities of the drone (i.e. $v_x$, $v_y$, $v_z$) will be perturbed by the thrust on the drone, which will be referred to as $u_3$. However, drone dynamics states that the thrust will only act in the $+z$ of the drone body-fixed frame, denoted by $X_b$, $Y_b$, and $Z_b$ in Figure \ref{fig:DroneFrame}. Thus, the thrust input must be rotated with respected to the roll, pitch, and yaw of the drone to find its effect along the $X_i$, $Y_i$, and $Z_i$ directions. An Euler 123 rotation, denoted by $R$ in Eq. \eqref{eq:R} is used to rotate from the body frame to the inertial frame\footnote{Throughout this report, $c$, $s$, and $t$ will be used in place of $\cos$, $\sin$, and $\tan$ for brevity}. Combining these steps, taking the time derivative of $X_4$ through $X_6$ gives Eq. \eqref{eq:X2dot}.

\begin{figure}[htb]
	\centering\includegraphics[width=0.8\textwidth]{Figures/DroneFrame}
	\caption{An illustration comparing the inertial world frame (denoted by dotted lines), and the body-fixed frame (denoted by solid lines)}
	\label{fig:DroneFrame}
\end{figure}

\begin{equation}
\label{eq:R}
R_{123} = R_z(\psi)R_y(\theta)R_x(\phi) = \begin{bmatrix}
c(\psi) & s(\psi) & 0 \\
s(\psi) & c(\psi) & 0 \\
0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
c(\theta) & 0 & s(\theta) \\
0 & 1 & 0 \\
-s(\theta) & 0 & c(\theta)
\end{bmatrix}
\begin{bmatrix}
1&0&0\\
0&c(\phi)&-s(\phi)\\
0&s(\phi)&c(\phi)
\end{bmatrix}
\end{equation}
\begin{equation}
R_{123}
=
\begin{bmatrix}
c(\theta)c(\psi) & c(\psi)s(\theta)s(\phi) - c(\phi)s(\psi) & s(\phi)s(\psi) + c(\phi)c(\psi)s(\theta) \\
c(\theta)s(\psi) & c(\phi)c(\psi) + s(\theta)s(\phi)s(\psi) & c(\phi)s(\theta)s(\psi) - c(\psi)s(\phi) \\
-s(\theta) & c(\theta)s(\phi) & c(\theta)c(\phi) 
\end{bmatrix}
\end{equation}

\begin{equation}
\label{eq:X2dot}
\begin{bmatrix}
\dot{v}_x\\
\dot{v}_y\\
\dot{v}_z\\
\end{bmatrix}
= \frac{R_{123}
\begin{bmatrix}
0\\
0\\
u_3
\end{bmatrix}
-
\begin{bmatrix}
0\\
0\\
mg
\end{bmatrix}}{m}=
\begin{bmatrix}
(u_3(s(\phi)s(\psi) + c(\phi)c(\psi)s(\theta)))/m \\
-(u_3(c(\psi)s(\phi) - c(\phi)s(\theta)s(\psi)))/m \\
-(mg - u_3c(\theta)c(\phi))/m
\end{bmatrix}
\end{equation}

Similarly, since $p$, $q$, and $r$ (i.e. the roll, pitch, and yaw rates) are in the body frame, they must be rotated into the inertial frame. Taking the time derivative of $X_7$ through $X_9$ then gives Eq. \eqref{eq:X3dot}.

\begin{equation}
\label{eq:X3dot}
\begin{bmatrix}
\dot{\phi}\\
\dot{\theta}\\
\dot{\psi}
\end{bmatrix}=
\begin{bmatrix}
1& s(\phi)t(\theta)&  c(\phi)t(\theta)\\
0& c(\phi)&          -s(\phi)\\
0& s(\phi)/c(\theta)&c(\phi)/c(\theta)
\end{bmatrix}
\begin{bmatrix}
p\\
q\\
r
\end{bmatrix}=
\begin{bmatrix}
p + r\cos(\phi)\tan(\theta) + q\tan(\theta)\sin(\phi) \\
q\cos(\phi) - r\sin(\phi) \\
(r\cos(\phi))/\cos(\theta) + (q\sin(\phi))/\cos(\theta)
\end{bmatrix}
\end{equation}

The rotational equations of motion can be derived from Euler’s equations for rigid body dynamics. Expressed in vector form, Euler’s equations are written as

\[
I\dot{\omega}+\omega\times(I\omega)=\tau
\]

where $\omega$ is the angular velocity vector, $I$ is the inertia matrix, and $\tau$ is a vector of external torques. This can be rewritten as
% COMBAK: look at this and see if the right variables are being used
\[
\dot{\omega}=\begin{bmatrix}
\dot{p}\\
\dot{q}\\
\dot{r}\\
\end{bmatrix}=
I^{-1}(\begin{bmatrix}
u_4\\u_5\\u_6
\end{bmatrix}\omega\times	(I\omega))
\]

where $u_4$ through $u_6$ are the roll, pitch, yaw torques on the drone respectively. The quadcopter can be modeled as two thin uniform rods crossed at the origin with a point mass (the motor) at the end of each. This results in a diagonal inertia matrix of the form

\[
I = \begin{bmatrix}
I_{xx}&0&0\\
0&I_{yy}&0\\
0&0&I_{zz}
\end{bmatrix}
\]

Combining these steps, the final result of the time derivative of states $X_{10}$ through $X_{12}$ can be seen in Eq. \eqref{eq:X4dot}\cite{X4dot}.

\begin{equation}
\label{eq:X4dot}
\begin{bmatrix}
\dot{p}\\
\dot{q}\\
\dot{r}
\end{bmatrix}=
\begin{bmatrix}
(u_4 + I_{yy}qr - I_{zz}qr)/I_{xx}\\
(u_5 - I_{xx}pr + I_{zz}pr)/I_{yy}\\
(u_6 + I_{xx}qp - I_{yy}qp)/I_{zz}
\end{bmatrix}
\end{equation}

% COMBAK: DELETE THIS LATER!!!!!!!!
Combining Equations \ref{eq:X1dot}, \ref{eq:X2dot}, \ref{eq:X3dot} and \ref{eq:X4dot} gives Eq. \eqref{eq:Xdot}.


\begin{equation}
\label{eq:Xdot}
\dot{X}(t) = F(X(t), t) = \begin{bmatrix}
\dot{x} \\ \dot{y} \\ \dot{z} \\ \dot{v_x} \\ \dot{v_y} \\ \dot{v_z} \\ \dot{\phi} \\ \dot{\theta} \\ \dot{\psi} \\ \dot{p} \\ \dot{q} \\ \dot{r} 
\end{bmatrix} =  \begin{bmatrix}
v_x \\
v_y \\
v_z \\
(u_3(\sin(\phi)\sin(\psi) + \cos(\phi)\cos(\psi)\sin(\theta))/m \\
(-u_3(\cos(\psi)\sin(\phi) + \cos(\phi)\sin(\theta)\sin(\psi))/m \\
(u_3\cos(\theta)\cos(\phi) - mg)/m \\
p + r\cos(\phi)\tan(\theta) + q\tan(\theta)\sin(\phi) \\
q\cos(\phi) - r\sin(\phi) \\
\frac{r\cos(\phi)}{\cos(\theta)} + \frac{q\sin(\phi)}{\cos(\theta)} \\
\frac{u_4 + qrI_{yy} - qrI_{zz}}{I_{xx}} \\
\frac{u_5 - prI_{xx} + prI_{zz}}{I_{yy}} \\
\frac{u_6 + qpI_{xx} - qpI_{yy}}{I_{zz}} \\
\end{bmatrix}
\end{equation}

Taking the partial of $F$ with respect to $X$ gives Eq. \eqref{eq:A}.
\begin{equation}
\label{eq:A}
A = \frac{\partial F}{\partial X} = 
\begin{bmatrix} 
    \frac{\partial F_1}{\partial x} & \dots  & \frac{\partial F_1}{\partial r}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial F_{12}}{\partial x} & \dots  & \frac{\partial F_{12}}{\partial r} 
\end{bmatrix}
\end{equation}

\begin{figure}[H]
	\centering\includegraphics[width=\textwidth]{Figures/AMatrix}
	\caption{The final A matrix, where $c$, $s$, and $t$, are $\cos$, $\sin$, and $\tan$. (This was included as a figure since \LaTeX\ did not allow matrices this wide)}
	\label{fig:AMatrix}
\end{figure}

Since the SDF of the Iris model specifies its physical properties to be used in by the simulator, the drone's mass and moment of inertia values were taken from the file to be $m = 1.545\ \text{kg}$, $I_{xx} = 0.029125\ \text{kg}\cdot \text{m}^2$, $I_{yy} = 0.029125\ \text{kg}\cdot \text{m}^2$, and $I_{zz} = 0.055225\ \text{kg}\cdot \text{m}^2$, and $g = 9.81\ \text{m}/\text{s}^2$ was used.

% TODO: change ref to ~ref
It can be noticed from Eq.~\ref{eq:Xdot} that to be able to propagate the drone's state forward, it is necessary to have some knowledge of $u_3$ through $u_6$, i.e. the thrust, and roll, pitch, yaw torques. A Linear–quadratic regulator (LQR) controller will be created for this purpose. The LQR controller will take in the drone's current state, compute $A$ using Eq.~\ref{eq:A} with $u_3 = mg$, compute $B$ using Eq.~\ref{eq:B}, and uses $Q$ and $R$ from Eq. \ref{eq:QandR} to compute the optimal gain matrix for the LQR controller, $K$, according to Eq. \ref{eq:LQR}, and return a vector of inputs $u_3$ through $u_6$ to control the drone\footnote{Note that $Q$, $R$, and $K$ in the LQR controller play a similar role as they do in the Kalman filter, but they are not the same value}. The LQR logic will be done using MATLAB's \texttt{lqr} function\cite{LQR}.

\begin{equation}
\begin{split}
\label{eq:B}
B &= \frac{\partial F}{\partial u} = 
\begin{bmatrix} 
    \frac{\partial F_1}{\partial u_3} & \dots  & \frac{\partial F_1}{\partial u_6}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial F_{12}}{\partial u_3} & \dots  & \frac{\partial F_{12}}{\partial u_6} 
\end{bmatrix}\\&=\begin{bmatrix}
         0&0&0&0\\
         0&0&0&0\\
         0&0&0&0\\
         (\sin(\phi)\sin(\psi) + \cos(\phi)\cos(\psi)\sin(\theta)) / m&0&0&0\\
         -(\cos(\psi)\sin(\phi) - \cos(\phi)\sin(\theta)\sin(\psi)) / m&0&0&0\\
         (\cos(\phi)\cos(\theta)) / m&0&0&0\\
         0&0&0&0\\
         0&0&0&0\\
         0&0&0&0\\
         0&I_{xx}^{-1}&0&0\\
         0&0&I_{yy}^{-1}&0\\
         0&0&0&I_{zz}^{-1}
\end{bmatrix}
\end{split}
\end{equation}

\begin{equation}
\label{eq:QandR}
\begin{split}
Q &= \texttt{diag([100 100 100 100 100 100 0.1 0.1 0.1 10 10 10])}\\
R &= \texttt{diag([0.1 1000 1000 1000])}
\end{split}
\end{equation}

\begin{equation}
\label{eq:LQR}
\begin{split}
&\text{Find}\  K\  \text{such that}\  u=-Kx\  \text{minimizes}
J(u)=\int_{0}^{\infty} (x^TQx+u^TRu)dt\\
&\text{subject to the system dynamics}\  \dot{x}=Ax+Bu
\end{split}
\end{equation}

\section{Measurement Model}

The distance from the camera to the centroid of the drone was measured by each of the infrared cameras, and roll, pitch, yaw measurements of the drone would directly be measured using the IMU. To find the distance from the cameras to the centroid, the cameras would return the average of the distances of each of the reflective markers from the cameras. This math is done implicitly within the Vicon computer software, and just the ranges are returned. Thus, instead of maintaining a separate state for each of the reflective markers on the drone, it will be assumed that the measurement model uses the coordinates of the centroid to measure the range to the centroid\footnote{The alternative to this is to maintain the state of each marker separately, and propagate each state forward individually. However, since the reflective markers are placed almost symmetrically across the axes of the drone-fixed frame, this will offer little to no improvement in the accuracy of the estimates, and that possible slight improvement in accuracy will not outweigh the increase in computational time}. The noise for the range measurements will be set as the claimed accuracy on the specsheet of the Vicon Vantage V16. The roll, pitch, and yaw measurements will be used directly from the IMU measurements. The noise for the roll, pitch, yaw measurements will be experimentally determined by find the variance of the IMU measurements when the drone is stationary. The measurement model was then formulated as in Eq. \ref{eq:G}, where $X_n$, $Y_n$, and $Z_n$ are the $X$, $Y$, and $Z$ coordinates of the $n$th camera.

\begin{equation}
\label{eq:G}
G(X(t), t) = 
\begin{bmatrix}
	\sqrt{(x-X_1)^2 + (y-Y_1)^2 + (z-Z_1)^2} \\
	\sqrt{(x-X_2)^2 + (y-Y_2)^2 + (z-Z_2)^2} \\
	\sqrt{(x-X_3)^2 + (y-Y_3)^2 + (z-Z_3)^2} \\
	\sqrt{(x-X_4)^2 + (y-Y_4)^2 + (z-Z_4)^2} \\
	\phi \\
	\theta \\
	\psi
\end{bmatrix}
\end{equation}

The observation matrix, $H$, was found by taking partial of the measurement model, $G$, with respect to $X$.

\begin{equation}
\label{eq:H}
H = \frac{\partial G}{\partial X} =
\begin{bmatrix} 
    \frac{\partial G_1}{\partial x} & \dots  & \frac{\partial G_1}{\partial r}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial G_{7}}{\partial x} & \dots  & \frac{\partial G_{7}}{\partial r}
\end{bmatrix} 
\end{equation}

\begin{figure}[htb]
	\centering\includegraphics[width=\textwidth]{Figures/HMatrix}
	\caption{The final H matrix where $X_n$, $Y_n$, and $Z_n$ are the $X$, $Y$, and $Z$ coordinates of the $n$th camera (This was included as a figure since \LaTeX\ did not allow matrices this wide)}
	\label{fig:HMatrix}
\end{figure}

\section{Filter Setup}

Using the aforementioned dynamics and measurement model, a hybrid Extended Kalman filter and an Unscented Kalman filter were implemented. The hybrid EKF algorithm that was used for this study can be seen in Algorithm \ref{alg:EKF}. The UKF algorithm used for this study can be seen in Algorithm \ref{alg:UKF}. The MATLAB implementation of these algorithms can be seen in the Appendix.

\begin{algorithm}[H]
\footnotesize
\caption{Hybrid Extended Kalman filter}
\label{alg:EKF}
\begin{algorithmic}[1]
\State Recognize system equations $\hat{x}=f(x,u,w,t), y_k=h_k(x_k,v_k), w(t)\sim(0,Q), v_k\sim(0,R)$
\State Initialize $x_{initial}$, $P$, $Q$, $R$
\State Set $\hat{x}_{k-1}^+ \gets x_{initial}$, $P_{k-1}^+ \gets P$
\For{each measurement $y$}
    \State Assemble $y_{obs}$ by reading in measurements at $t_k$
    \State $dt \gets t_k - t_{k-1}$
    \State $\hat{x}_k^- \gets$ propagate $\hat{x}_{k-1}^+$ using the dynamics $\dot{\hat{x}}_k$ over $dt$ using ode45
    \State $A \gets$ assemble according to Eq. \ref{eq:A}
    \State $\dot{P} \gets AP_{k-1}^+ + P_{k-1}^+A^T + Q$
    \State Assemble $y_{comp}$ according to Eq. \ref{eq:G} using $\hat{x}_k^-$
    \State Assemble $H_k$ according to Eq. \ref{eq:H} using $\hat{x}_k^-$ and coordinates in Table \ref{tab:cameras}
    \State $P_k^- \gets P_{k-1}^+ + \dot{P}\times dt$
    \State $K_k \gets P_k^- H_k^T(H_kP_k^-H_k^T+R)^{-1}$
    \State $\hat{x}_k^+ \gets \hat{x}_k^- + K_k(y_{obs}-y_{comp}$
    \State $P_k^+\gets (I_{12}-K_kH_k)P_k^-(I_{12}-K_kH_k)^T + R$
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\footnotesize
\caption{Unscented Kalman filter}
\label{alg:UKF}
\begin{algorithmic}[1]
\State Recognize system equations $x_{k+1}=f(x_k,u_k,t_k)+w_k, y_k=h(x_k,t_k)+v_k, w(t)\sim(0,Q_k), v_k\sim(0,R_k)$
\State Initialize $x_{initial}$, $P$, $Q$, $R$
\State Set $\hat{x}_{k-1}^+ \gets x_{initial}$, $P_{k-1}^+ \gets P$, $n \gets$ number of states
\For{each measurement $y$}
    \State Assemble $y_{obs}$ by reading in measurements at $t_k$
    \State $dt \gets t_k - t_{k-1}$
    \For{j=1:2n}
    		\State $\hat{x}_{k-1}^{(i)} \gets \hat{x}_{k-1}^+ + \tilde{x}^{(i)}; \quad i=1,\cdots,2n$
    		\State $\tilde{x}^{(i)} \gets (\sqrt{nP_{k-1}^+})_i^T; \quad i=1,\cdots,n$
    		\State $\tilde{x}^{(n+i)} \gets -(\sqrt{nP_{k-1}^+})_i^T; \quad i=1,\cdots,n$
    	\EndFor
    \State $\hat{x}_k^{(i)} \gets$ propagate $\hat{x}_{k-1}^{(i)}$ using the dynamics $\dot{\hat{x}}_k^{(i)}$ over $dt$ using ode45
    \State $\hat{x}_k^- \gets \frac{1}{2n}\sum_{i=1}^{2n}\hat{x}_k^{(i)}$
    \State $P_k^- \gets \frac{1}{2n}\sum_{i=1}^{2n}(\hat{x}_k^{(i)}-\hat{x}_k^-)(\hat{x}_k^{(i)}-\hat{x}_k^-)^T + Q_{k-1}$
    \For{j=1:2n}
    		\State $\hat{x}_{k}^{(i)} \gets \hat{x}_{k}^+ + \tilde{x}^{(i)}; \quad i=1,\cdots,2n$
    		\State $\tilde{x}^{(i)} \gets (\sqrt{nP_{k}^+})_i^T; \quad i=1,\cdots,n$
    		\State $\tilde{x}^{(n+i)} \gets -(\sqrt{nP_{k}^+})_i^T; \quad i=1,\cdots,n$
    	\EndFor
    \State Assemble $\hat{y}_{k}^{(i)}$ according to Eq. \ref{eq:G} using $\hat{x}_k^{(i)}$
    \State $\hat{y}_k \gets \frac{1}{2n}\sum_{i=1}^{2n}\hat{y}_k^{(i)}$
    \State $P_y \gets \frac{1}{2n}\sum_{i=1}^{2n}(\hat{y}_k^{(i)}-\hat{y}_k)(\hat{y}_k^{(i)}-\hat{y}_k)^T + R_k$
    \State $P_{xy} \gets \frac{1}{2n}\sum_{i=1}^{2n}(\hat{x}_k^{(i)}-\hat{x}_k^-)(\hat{y}_k^{(i)}-\hat{y}_k)^T$
    \State $K_k \gets P_{xy}P_y^{-1}$
    \State $\hat{x}_k^+ \gets \hat{x}_k^- + K_k(y_{obs}-\hat{y}_k)$
    \State $P_k^+ \gets P_k^- - K_kP_yK_k^T$
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Experimental Procedure}

The range measurements from the cameras and the roll, pitch, yaw measurements from the IMU were stored as ROSbags, and were read into MATLAB. The a priori state was measured during every experiment. The a priori state covariance, $P_0^+$ was set to \codeword{diag([(0.001)^2 (0.001)^2 (0.001)^2 0 0 0 (3.8785e-5)^2 (3.8785e-5)^2 (3.8785e-5)^2 0 0 0])}, since it is possible that there were errors on the scale of millimeters when measuring the starting position of the drone, $3.8785\cdot 10^{-5}$ radians of error from the IMU measurements (according to the standard deviation of the measurements when the drone was at rest), and 0 error in the velocities and angular rates since it was at rest. The variance of the Vicon Vantage V16 motion capture cameras was specified to be $1.5$ mm in their specsheet \cite{vicon_2022}. However, such small variances did not show how robust the filters' to non-ideal conditions. Hence, the variances of each of the four cameras' range measurements was set as $0.0015^2$ m, $0.015^2$ m, $0.002^2$ m, and $0.1^2$ m to showcase the filters' robustness, and the aforementioned $3.8785\cdot 10^{-5}$ radians of error was used for the IMU measurements. Hence, the covariance matrix of the measurements was set as \codeword{R = diag([0.0015^2 0.015^2 0.002^2 0.1^2 (3.8785e-5)^2 (3.8785e-5)^2 (3.8785e-5)^2])}.

Four types of trajectories were used to analyze the performance of the filters for this study:
\begin{enumerate}[label=(\alph*)]
\item Hover - The drone hovers to a height of 3 m and lands (as seen in Figure \ref{fig:hover1_just_traj})
\item Circle - The drone hovers to a height of 3 m and starts flying in a circle with radius 1 m (as seen in Figure \ref{fig:circle1_just_traj})
\item Sine - The drone hovers to a height of 3 m and flies in a sine wave in the +x-direction (as seen in Figure \ref{fig:sine1_just_traj})
\item Square - The drone hovers to a height of 3 m and flies in a square with a sidelength of 4 m (as seen in Figure \ref{fig:square1_just_traj})
\end{enumerate}

% TODO: change the size of this later
\begin{figure}[H]
\centering
\begin{subfigure}{.3\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../circle1_just_traj}
	\caption{Hover trajectory!!!}
	\label{fig:hover1_just_traj}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../circle1_just_traj}
	\caption{Circle trajectory}
	\label{fig:circle1_just_traj}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../circle1_just_traj}
	\caption{Sine trajectory!!!}
	\label{fig:sine1_just_traj}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{../circle1_just_traj}
	\caption{Square trajectory!!!}
	\label{fig:square1_just_traj}
\end{subfigure}
\caption{The trajectories that the drone followed}
\label{fig:DroneTrajectories}
\end{figure}

The default sampling rate of the cameras was at 120 Hz, but the stored data was modified to replicate a measurement rate of 60 Hz, 24 Hz, 12 Hz, 6 Hz, and an extreme of 1 Hz, and the performance of the filters over these measurement rates was compared.

\section{Results and Discussion}

The results of the hybrid Extended Kalman filter will be discussed first. The standard deviations of errors in position estimates for the different trajectories at various sampling rates can be seen in Tables \ref{tab:sigmax} through \ref{tab:sigmaz}.

% TODO discuss Q

\begin{table}[htb]
\centering
\caption{Standard deviation of x-position error}
\label{tab:sigmax}
\begin{tabular}{|r|llll|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\backslashbox{Sample Rate}{Trajectory Type}}} & \multicolumn{4}{c|}{$\sigma_x$ (in m)}                                                                            \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                                                             & \multicolumn{1}{c}{Hover} & \multicolumn{1}{c}{Circle} & \multicolumn{1}{c}{Square} & \multicolumn{1}{c|}{Sine} \\ \hline
120 Hz                                                                             & 0.0039932                 & 0.0085408                  & 0.0080575                  & 0.0049977                 \\
60 Hz                                                                              & 0.0051788                 & 0.010278                   & 0.0096354                  & 0.0057264                 \\
24 Hz                                                                              & 0.0056789                 & 0.013199                   & 0.011393                   & 0.0074061                 \\
12 Hz                                                                              & 0.0074068                 & 0.014731                   & 0.012044                   & 0.0085448                 \\
6 Hz                                                                               & 0.0081106                 & 0.016047                   & 0.012508                   & 0.0087954                 \\
1 Hz                                                                               & 0.0095391                 & 0.029615                   & 0.015374                   & 0.013841                  \\ \hline
\end{tabular}
\end{table}

\begin{table}[htb]
\centering
\caption{Standard deviation of y-position error}
\label{tab:sigmay}
\begin{tabular}{|r|llll|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\backslashbox{Sample Rate}{Trajectory Type}}} & \multicolumn{4}{c|}{$\sigma_y$ (in m)}                                                                          \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                                                             & \multicolumn{1}{c}{Hover} & \multicolumn{1}{c}{Circle} & \multicolumn{1}{c}{Square} & \multicolumn{1}{c|}{Sine} \\ \hline
120 Hz                                                                             & 0.0040409                 & 0.0084944                  & 0.0080132                  & 0.0049065                 \\
60 Hz                                                                              & 0.0052825                 & 0.010371                   & 0.0095661                  & 0.005823                  \\
24 Hz                                                                              & 0.0056995                 & 0.012979                   & 0.011269                   & 0.0074443                 \\
12 Hz                                                                              & 0.007463                  & 0.014687                   & 0.011893                   & 0.0089392                 \\
6 Hz                                                                               & 0.0084155                 & 0.015925                   & 0.012634                   & 0.0087636                 \\
1 Hz                                                                               & 0.0086878                 & 0.02909                    & 0.015167                   & 0.013621                  \\ \hline
\end{tabular}
\end{table}

\begin{table}[htb]
\centering
\caption{Standard deviation of z-position error}
\label{tab:sigmaz}
\begin{tabular}{|r|llll|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{\backslashbox{Sample Rate}{Trajectory Type}}} & \multicolumn{4}{c|}{$\sigma_z$ (in m)}                                                                          \\ \cline{2-5} 
\multicolumn{1}{|l|}{}                                                             & \multicolumn{1}{c}{Hover} & \multicolumn{1}{c}{Circle} & \multicolumn{1}{c}{Square} & \multicolumn{1}{c|}{Sine} \\ \hline
120 Hz                                                                             & 0.0031726                 & 0.0030319                  & 0.0035243                  & 0.010083                  \\
60 Hz                                                                              & 0.0028092                 & 0.0032654                  & 0.0040193                  & 0.012393                  \\
24 Hz                                                                              & 0.0024405                 & 0.0036128                  & 0.004693                   & 0.015404                  \\
12 Hz                                                                              & 0.0027021                 & 0.0046044                  & 0.0048127                  & 0.019746                  \\
6 Hz                                                                               & 0.0024644                 & 0.0041818                  & 0.0056552                  & 0.017945                  \\
1 Hz                                                                               & 0.01566                   & 0.01327                    & 0.0088481                  & 0.030748                  \\ \hline
\end{tabular}
\end{table}

It can be seen from Tables \ref{tab:sigmax} through \ref{tab:sigmaz} that the error in actual position and estimated position increases for all trajectories as the sampling rate is decreased. This is intuitive because as the resolution of data decreases, the system has to use its knowledge of the system's dynamics to propagate itself to the next state, and this may differ from the system's real-life behavior since many approximations were made to linearize the system's dynamics. This also explains why the magnitude of error in the z-direction is lower than the error in the x and y directions, since the drone does not move much in the z directions while performing the trajectory, therefore only causing small deviations from the actual. (WHAT) That being said, the filter is still accurate to a hundredth of a meter, and having centimeter level accuracy is incredible when considering that the errors in the measurements from the motion capture cameras were greatly exaggerated (used $1.5$ mm, $1.5$ cm, $2$ mm, and $10$ cm in each of the four cameras instead of the rated values of $1.5$ mm\cite{vicon_2022}).

\section{Conclusion}


\section{Acknowledgment}

Thanks to Dr. Brian Gunter for a great semester. I'd heard of Kalman filters through my work in the past, but I hadn't known how they worked. The PhD student I was working with during my last semester at UT Austin's Autonomous Systems Group told me I'd learn about them in graduate school, and I got to learn about them from Dr. Gunter. I learned a lot through this class, and I'm looking forward to working more intricately with Kalman filters. 

\appendix
\section*{Appendix: Title here}


\bibliographystyle{AAS_publication}   % Number the references.
\bibliography{references}   % Use references.bib to resolve the labels.


\end{document}
